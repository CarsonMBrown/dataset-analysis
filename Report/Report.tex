\documentclass[12pt,a4paper,final]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{sectsty}
\allsectionsfont{\centering}
\usepackage{textcomp}
\usepackage{pdfpages}
\usepackage{graphicx}
\graphicspath{{./Images/}}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{forest}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=blue,
    pdfborderstyle={/S/U/W 1}
}

\definecolor{folderbg}{RGB}{124,166,198}
\definecolor{folderborder}{RGB}{110,144,169}

\begin{document}
\title{%
    CS2006: Python 2 \\
    \Large Dataset Analysis With Pandas} \\
\author{, 180000269, 180008859 Tutor: Stephen Linton}
\date{May 07, 2020}
\maketitle

\section*{Overview}
In this practical we are asked to use Python Data Analysis Library (Pandas) to analyse Wikipedia citation data of four languages: German, English, Chinese and Ukrainian.

The raw data of each language is presented in a TSV (tab-separated values) file.

Before starting detailed analysis of each language, verification and pre-processing of the raw data are very important first steps.

After raw data has been read in correctly and cleaned, initial descriptive analysis is performed. Finally, additional analytics are performed to explore the data further.

\section*{Design}
Pre-processing raw data ... (TODO: cleandata.py structure)

Analytics take form in modular functions, using pandas dataframe as parameter, and results of analysis are outputted in a series of print statements.

Extension work has been designed similarly as modular functions, runs in series after initial analysis.

\subsection*{Base Specifications}

The minimum requirements of this project were to implement the parts which were currently undefined
to complete

We were specifically asked for the following features:

\begin{itemize}[noitemsep]
    \item Check consistency of initial raw data
    \item Refine data in case of inconsistencies before analysis
    \item Output total number of records
    \item Output range of dates represented in the data
    \item Output a table with number of records for each identifier type
\end{itemize}

\section*{Design}

\subsection*{Base Requirements}

We fully completed all the requirements of the base practical.

Each subsection for the additional requirements includes a bulleted list of
extra features that fit within that difficulty level.

NB: As some requirements build on others, the features listed as the difficulty goes
up may overlap with those listed earlier to make sure that all the features are mentioned,
especially if they were specific to a suggested requirement difficulty.

\subsection*{Easy, Medium, and Hard Requirements}
\begin{center} \emph{Easy} \end{center}
\begin{itemize}[noitemsep]
    \item Output a table with percentage of records for each identifier type
    \item Calculate an average number of days since citation appeared on Wikipedia
    \item Output a table with the number of citations from arXiv by the year of their appearance.
\end{itemize}

\begin{center} \emph{Medium} \end{center}
\begin{itemize}[noitemsep]
    \item Find an average number of citations per page
    \item Find first ten pages citing the largest number of sources
    \item Output a table with the number of citations from Zenodo by the year of their appearance
\end{itemize}

\begin{center} \emph{Hard} \end{center}
\begin{itemize}[noitemsep]
    \item Find first ten most highly cited sources
    \item Output a table with the number and percentage of citations appearing on Wikipedia by the number of years since their appearance
    \item Analyse and optimise the performance of different steps of your analysis (see Evaluation section below)
\end{itemize}

\section*{Evaluation}
Parsing timestamp column to date\_time format rather than object format has helped to optimise further analysis, because it is arduous and memory-inefficent to work with date\_time as strings.

While initially we parse timestamp column to datatime during the pre-processing stage, we realized that after some research there is a more efficient way to parse ISO 8601 format in the \code{read\_csv} stage.
In fact, one of the parameter of \code{read\_csv}, \code{parse\_dates}, specially mentions that it has a fast route for ISO 8601 formatted data.

Other important high performance Pandas function usage includes \code{groupby} and \code{value\_counts}. On the one hand, \code{groupby} has been efficient in operating data relating one column to other columns.
\code{value\_counts}, on the other hand, focuses on finding frequencies of values in a single column.

Moreover, Pandas \code{.apply()} function, which is a powerful replacement of iterative functions, has been used in Hard extensions.

Overall, we are very happy with the progress and completeness of our solution to both the base and additional requirements. Our \code{.apply()} takes another function as its input and applies it along an axis of a DataFrame.
In such cases where we are passing functions, a lambda is often convenient to package everything together in an optimised manner.

\section*{Conclusion}

To begin with there was a lack of structure and people worked on the project individually and occasionally pushed to github. We didn't begin our work for several weeks, and once we did begin work, it was in disparate intervals, such that no one in our group was really able to work on the project at the same time due to timezones and individual scheduling of time.

We would have liked to be able to implement all the extensions, but we didn't have the time to make sure everyone would be able to do a similar amount of work while still completing all the extensions.

We would have liked to generate a better way of displaying the data, as textual output is only so good at showing results, especially when the results are so inclined to being displayed via graphical displays such as bar graphs.

\section*{Provenance}

We did not source any code from anywhere and wrote all the code submitted ourselves. We did use examples and guides to figure out our code structure, but no code was not written by us.

\section*{Citations}

\noindent
\href{https://www.w3resource.com/pandas/series/series-value_counts.php}{values\_counts function and parameter usage}
\newline - Used some parameters such as normalize and ascending to sort resultant dataframe

\noindent
\href{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html}{read\_csv documentation of Pandas}
\newline - Used to parse datetime at input stage, optimise performance

\noindent
\href{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html}{use pandas to parse a column to datetime}
\newline - Used to analyse timestamp column efficiently

\noindent
\href{https://docs.python.org/3/library/datetime.html} {understand python datetime}
\newline - Used to extract year, date from timestamp column

\noindent
\href{https://books.google.co.uk/books?id=LcHgDwAAQBAJ&pg=PT337&lpg=PT337&dq=pandas+value_counts+performance&source=bl&ots=bgNWdiwkVC&sig=ACfU3U0bGo5e7tdN6bztLRuGNm3EC1vI6Q&hl=en&sa=X&ved=2ahUKEwjzpMjT9JzpAhXDTxUIHXdJDBgQ6AEwBnoECAoQAQ#v=onepage&q=pandas%20value_counts%20performance&f=false} {Pandas high performance functions}
\newline - Referenced to analyse optimisation when using groupby and value\_counts functions
\end{document}
